"""
FastAPI Server for PaddleOCR-VL with OpenAI-compatible API

This server provides OCR capabilities for PDF documents using PaddleOCR-VL,
a state-of-the-art vision-language model optimized for document parsing.

Endpoints:
    - POST /ocr: Direct OCR endpoint (PDF file upload)
    - POST /v1/chat/completions: OpenAI-compatible endpoint (base64 PDF)
    - GET /health: Health check endpoint
    - GET /: Service information

Features:
    - Intelligent blank page detection (ROI-based variance analysis)
    - GPU acceleration (CUDA support)
    - Automatic image optimization
    - Markdown output format
    - Detailed processing statistics

Environment Variables:
    - MODEL_ID: HuggingFace model ID (default: PaddlePaddle/PaddleOCR-VL)
    - MAX_NEW_TOKENS: Maximum tokens for generation (default: 2048)
    - PDF_DPI: PDF rendering resolution (default: 200)
    - PORT: Server port (default: 8080)
"""

import os
import io
import logging
import base64
from typing import Optional, List, Dict, Any
from contextlib import asynccontextmanager

import torch
import uvicorn
from fastapi import FastAPI, File, UploadFile, HTTPException, Request
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from transformers import AutoModelForCausalLM, AutoProcessor
from PIL import Image

from pdf_processor import PDFProcessor, images_to_markdown

# Logging configuration
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Model configuration
MODEL_ID = os.getenv("MODEL_ID", "PaddlePaddle/PaddleOCR-VL")  # HuggingFace model ID
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"  # Auto-detect GPU
MAX_NEW_TOKENS = int(os.getenv("MAX_NEW_TOKENS", "2048"))  # Max generation length
DPI = int(os.getenv("PDF_DPI", "200"))  # PDF rendering resolution (200-300 optimal for OCR)

# Global variables for model and processor (loaded at startup)
model = None  # PaddleOCR-VL model instance
processor = None  # Tokenizer and image processor
pdf_processor = None  # PDF to image converter


# Pydantic models for OpenAI-compatible API
class Message(BaseModel):
    role: str
    content: str


class ChatCompletionRequest(BaseModel):
    model: str = Field(default="paddleocr-vl")
    messages: List[Message]
    temperature: Optional[float] = Field(default=0.0, ge=0.0, le=2.0)
    max_tokens: Optional[int] = Field(default=2048, ge=1)
    stream: Optional[bool] = Field(default=False)


class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Dict[str, int]


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Manages server startup and shutdown lifecycle.
    
    Startup: Loads model, processor, and PDF processor into memory
    Shutdown: Cleans up resources and frees GPU memory
    """
    # Startup: load model and processors
    global model, processor, pdf_processor
    
    logger.info(f"üöÄ Starting server on device: {DEVICE}")
    logger.info(f"üì¶ Loading model: {MODEL_ID}")
    
    try:
        # Load processor (tokenizer + image processor)
        processor = AutoProcessor.from_pretrained(
            MODEL_ID,
            trust_remote_code=True
        )
        logger.info("‚úÖ Processor loaded")
        
        # Load model (PaddleOCR-VL vision-language model)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if DEVICE == "cuda" else torch.float32,  # FP16 on GPU, FP32 on CPU
            low_cpu_mem_usage=True  # Optimize memory usage
        ).to(DEVICE)
        
        model.eval()  # Set to inference mode (disable dropout, etc.)
        logger.info(f"‚úÖ Model loaded on {DEVICE}")
        
        # Initialize PDF processor
        pdf_processor = PDFProcessor(dpi=DPI)
        logger.info("‚úÖ PDF Processor initialized")
        
        logger.info("üéâ Server ready to accept requests")
        
    except Exception as e:
        logger.error(f"‚ùå Error loading model: {e}")
        raise
    
    yield  # Server is running
    
    # Shutdown: cleanup resources
    logger.info("üõë Shutting down server")
    if model is not None:
        del model
    if processor is not None:
        del processor
    torch.cuda.empty_cache() if torch.cuda.is_available() else None  # Free GPU memory


# Initialize FastAPI with lifespan management
app = FastAPI(
    title="PaddleOCR-VL API",
    description="OpenAI-compatible API for PDF OCR with PaddleOCR-VL",
    version="1.0.0",
    lifespan=lifespan
)


@app.get("/health")
async def health_check():
    """
    Health check endpoint for Cloud Run and monitoring.
    
    Returns service status, model loading state, and device information.
    """
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "device": DEVICE,
        "cuda_available": torch.cuda.is_available()
    }


@app.get("/")
async def root():
    """
    Root endpoint with service information.
    
    Returns available endpoints and service metadata.
    """
    return {
        "service": "PaddleOCR-VL API",
        "version": "1.0.0",
        "model": MODEL_ID,
        "endpoints": {
            "chat_completions": "/v1/chat/completions",
            "ocr_direct": "/ocr",
            "health": "/health"
        }
    }


def run_ocr_on_image(image: Image.Image, task: str = "ocr") -> str:
    """
    Runs OCR on a single image using PaddleOCR-VL vision-language model.
    
    This function uses the official PaddleOCR-VL chat template format for inference.
    The model supports multiple document understanding tasks beyond basic OCR.
    
    Args:
        image: PIL Image in RGB format
        task: Task type - one of:
            - 'ocr': General text recognition (default)
            - 'table': Table structure recognition
            - 'formula': Mathematical formula recognition
            - 'chart': Chart and diagram recognition
        
    Returns:
        Extracted text from the image
        
    Raises:
        HTTPException: If OCR processing fails
    """
    try:
        # Official prompts from PaddleOCR-VL documentation
        # These prompts guide the model's behavior for different tasks
        PROMPTS = {
            "ocr": "OCR:",
            "table": "Table Recognition:",
            "formula": "Formula Recognition:",
            "chart": "Chart Recognition:",
        }
        
        # Convert image to RGB if necessary (model requires RGB input)
        if image.mode != "RGB":
            image = image.convert("RGB")
        
        # Prepare message in chat template format
        # PaddleOCR-VL uses a vision-language chat interface
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": PROMPTS.get(task, "OCR:")},
                ]
            }
        ]
        
        # Apply chat template and tokenize
        # This converts the message into model input format
        inputs = processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,  # Add assistant prompt
            return_dict=True,
            return_tensors="pt"  # PyTorch tensors
        ).to(DEVICE)
        
        # Run inference (no gradient computation needed)
        with torch.inference_mode():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,  # Maximum output length
                do_sample=False,  # Deterministic generation (no sampling)
                use_cache=True  # Enable KV cache for faster generation
            )
        
        # Decode generated tokens to text
        generated_text = processor.batch_decode(
            generated_ids,
            skip_special_tokens=True  # Remove <s>, </s>, etc.
        )[0]
        
        return generated_text
        
    except Exception as e:
        logger.error(f"OCR error: {e}")
        raise HTTPException(status_code=500, detail=f"OCR error: {str(e)}")


@app.post("/ocr")
async def ocr_endpoint(file: UploadFile = File(...), skip_blank: bool = True):
    """
    Direct OCR endpoint for PDF files.
    
    This endpoint processes PDF documents page by page, with intelligent blank page
    detection to reduce processing time and costs. Uses ROI-based variance analysis
    to identify pages with meaningful content.
    
    Args:
        file: PDF file to process (multipart/form-data)
        skip_blank: If True, skip blank/uniform pages (default: True)
                   Reduces processing time by ~31% on typical documents
        
    Returns:
        JSON response with:
            - success: Boolean indicating success
            - pages_total: Total number of pages in PDF
            - pages_processed: Number of pages with content
            - pages_skipped: Number of blank pages skipped
            - skipped_pages: List of skipped page numbers
            - processed_pages: List of processed page numbers
            - markdown: Extracted text in Markdown format
            - total_chars: Total character count
            - page_stats: Detailed statistics for each page (variance, brightness, etc.)
    
    Raises:
        HTTPException: 400 if file is not PDF, 500 if processing fails
    """
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(status_code=400, detail="Only PDF files are supported")
    
    try:
        # Read PDF file
        pdf_bytes = await file.read()
        logger.info(f"üìÑ Received PDF: {file.filename} ({len(pdf_bytes)} bytes)")
        
        # Convert PDF to images (with blank page filter)
        # Returns: images, page numbers (1-indexed), and detailed statistics
        images, page_numbers, page_stats = pdf_processor.pdf_to_images(
            pdf_bytes, 
            skip_blank=skip_blank
        )
        
        total_pages = len(page_stats) if page_stats else len(images)
        skipped_pages = total_pages - len(images)
        
        logger.info(
            f"üñºÔ∏è  Extracted {len(images)}/{total_pages} pages "
            f"(skipped {skipped_pages})"
        )
        
        # Run OCR on each page with content
        ocr_results = []
        for idx, (image, page_num) in enumerate(zip(images, page_numbers), start=1):
            logger.info(f"üîç Processing page {page_num} ({idx}/{len(images)})")
            
            # Optimize image for OCR (resize if too large)
            optimized_image = pdf_processor.optimize_image_for_ocr(image)
            
            # Run OCR
            text = run_ocr_on_image(optimized_image)
            ocr_results.append(text)
            logger.info(f"‚úÖ Page {page_num} completed ({len(text)} chars)")
        
        # Convert to Markdown format
        markdown_text = images_to_markdown(ocr_results)
        
        # Identify skipped page numbers
        skipped_page_numbers = []
        if page_stats:
            skipped_page_numbers = [
                stat['page_number'] 
                for stat in page_stats 
                if not stat['has_content']
            ]
        
        return {
            "success": True,
            "pages_total": total_pages,
            "pages_processed": len(images),
            "pages_skipped": skipped_pages,
            "skipped_pages": skipped_page_numbers,
            "processed_pages": page_numbers,
            "markdown": markdown_text,
            "total_chars": len(markdown_text),
            "page_stats": page_stats if page_stats else None
        }
        
    except Exception as e:
        logger.error(f"‚ùå Processing error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """
    OpenAI-compatible chat completions endpoint.
    
    This endpoint accepts PDF documents encoded in base64 format, following
    the OpenAI API specification. Useful for integration with existing tools
    that expect OpenAI-style APIs.
    
    Request format:
    {
        "model": "paddleocr-vl",
        "messages": [
            {
                "role": "user",
                "content": "data:application/pdf;base64,<base64_pdf>"
            }
        ],
        "temperature": 0.0,
        "max_tokens": 2048
    }
    
    Response format (OpenAI-compatible):
    {
        "id": "chatcmpl-<timestamp>",
        "object": "chat.completion",
        "created": <timestamp>,
        "model": "paddleocr-vl",
        "choices": [{
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "<markdown_text>"
            },
            "finish_reason": "stop"
        }],
        "usage": {
            "prompt_tokens": <estimated>,
            "completion_tokens": <word_count>,
            "total_tokens": <sum>
        }
    }
    """
    try:
        # Extract message content
        if not request.messages:
            raise HTTPException(status_code=400, detail="No messages provided")
        
        user_message = request.messages[-1]  # Get last user message
        content = user_message.content
        
        # Verify it's a base64-encoded PDF
        if not content.startswith("data:application/pdf;base64,"):
            raise HTTPException(
                status_code=400,
                detail="Content must be a base64-encoded PDF (data:application/pdf;base64,...)"
            )
        
        # Decode base64 PDF
        base64_data = content.split(",", 1)[1]  # Remove data URI prefix
        pdf_bytes = base64.b64decode(base64_data)
        logger.info(f"üìÑ Received PDF via OpenAI API ({len(pdf_bytes)} bytes)")
        
        # Convert PDF to images (with blank page filter enabled)
        images, page_numbers, page_stats = pdf_processor.pdf_to_images(
            pdf_bytes, 
            skip_blank=True  # Always skip blank pages for efficiency
        )
        
        total_pages = len(page_stats) if page_stats else len(images)
        logger.info(f"üñºÔ∏è  Extracted {len(images)}/{total_pages} pages")
        
        # Run OCR on each page with content
        ocr_results = []
        for idx, (image, page_num) in enumerate(zip(images, page_numbers), start=1):
            logger.info(f"üîç Processing page {page_num} ({idx}/{len(images)})")
            optimized_image = pdf_processor.optimize_image_for_ocr(image)
            text = run_ocr_on_image(optimized_image)
            ocr_results.append(text)
        
        # Convert to Markdown format
        markdown_text = images_to_markdown(ocr_results)
        
        # Build OpenAI-compatible response
        import time
        response = {
            "id": f"chatcmpl-{int(time.time())}",  # Unique completion ID
            "object": "chat.completion",
            "created": int(time.time()),  # Unix timestamp
            "model": request.model,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": markdown_text  # OCR result as assistant message
                    },
                    "finish_reason": "stop"  # Completed normally
                }
            ],
            "usage": {
                "prompt_tokens": len(images) * 100,  # Estimate: ~100 tokens per image
                "completion_tokens": len(markdown_text.split()),  # Word count
                "total_tokens": len(images) * 100 + len(markdown_text.split())
            }
        }
        
        return response
        
    except HTTPException:
        raise  # Re-raise HTTP exceptions as-is
    except Exception as e:
        logger.error(f"‚ùå Processing error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    # Get port from environment (Cloud Run sets PORT automatically)
    port = int(os.environ.get("PORT", 8080))
    
    # Run uvicorn server
    # host="0.0.0.0" allows external connections (required for Cloud Run)
    uvicorn.run(
        app,
        host="0.0.0.0",  # Listen on all network interfaces
        port=port,
        log_level="info",  # INFO level logging
        access_log=True  # Log all HTTP requests
    )
